  0%|                                                                                                                                                                                  | 0/1860 [00:00<?, ?it/s]Traceback (most recent call last):
Model inputs:
  pixel_values shape: torch.Size([1, 2916, 1176])
  input_ids shape: torch.Size([1, 24])
  labels shape: torch.Size([1, 48])
  attention_mask shape: torch.Size([1, 24])

Image grid THW: tensor([[ 1, 54, 54]], device='cuda:0')

Pixel values: tensor([[[ 0.5581,  0.5727,  0.6165,  ...,  1.0367,  1.0225,  1.1221],
         [ 0.5143,  0.5289,  0.5435,  ..., -0.9256, -0.8688, -0.7550],
         [ 0.6019,  0.6019,  0.6019,  ...,  1.1221,  1.1078,  1.1363],
         ...,
         [-0.3324, -0.3762, -0.3762,  ..., -0.2146, -0.2573, -0.2004],
         [-0.3178, -0.3178, -0.3470,  ..., -0.2004, -0.1435, -0.1293],
         [-0.4054, -0.4200, -0.4346,  ..., -0.2573, -0.2573, -0.2004]]],
       device='cuda:0')
  File "/home/zl3466/Documents/cmu/10623/proj/train.py", line 123, in <module>
    main()
  File "/home/zl3466/Documents/cmu/10623/proj/train.py", line 118, in main
    trainer.train()
  File "/home/zl3466/Documents/cmu/10623/proj/src/vo_trainer.py", line 270, in train
    trainer.train()
  File "/home/zl3466/anaconda3/envs/voformer/lib/python3.9/site-packages/transformers/trainer.py", line 2325, in train
    return inner_training_loop(
  File "/home/zl3466/anaconda3/envs/voformer/lib/python3.9/site-packages/transformers/trainer.py", line 2674, in _inner_training_loop
    tr_loss_step = self.training_step(model, inputs, num_items_in_batch)
  File "/home/zl3466/anaconda3/envs/voformer/lib/python3.9/site-packages/transformers/trainer.py", line 4020, in training_step
    loss = self.compute_loss(model, inputs, num_items_in_batch=num_items_in_batch)
  File "/home/zl3466/Documents/cmu/10623/proj/src/vo_trainer.py", line 40, in compute_loss
    outputs = model(**inputs)
  File "/home/zl3466/anaconda3/envs/voformer/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/zl3466/anaconda3/envs/voformer/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/zl3466/Documents/cmu/10623/proj/src/vo_model.py", line 65, in forward
    outputs = self.base_model(
  File "/home/zl3466/anaconda3/envs/voformer/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/zl3466/anaconda3/envs/voformer/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/zl3466/anaconda3/envs/voformer/lib/python3.9/site-packages/transformers/models/qwen2_5_vl/modeling_qwen2_5_vl.py", line 1259, in forward
    image_mask, _ = self.get_placeholder_mask(
  File "/home/zl3466/anaconda3/envs/voformer/lib/python3.9/site-packages/transformers/models/qwen2_5_vl/modeling_qwen2_5_vl.py", line 1202, in get_placeholder_mask
    raise ValueError(
ValueError: Image features and image tokens do not match: tokens: 0, features 729
