  0%|                                                                                                                                                                                  | 0/1860 [00:00<?, ?it/s]Traceback (most recent call last):
Number of input images: 9
Processed pixel_values shape: torch.Size([2916, 1176])

Number of input images: 9
Processed pixel_values shape: torch.Size([2916, 1176])

Model inputs:
  pixel_values shape: torch.Size([1, 2916, 1176])
  input_ids shape: torch.Size([1, 24])
  labels shape: torch.Size([1, 48])
  attention_mask shape: torch.Size([1, 24])

Image grid THW: tensor([[ 1, 54, 54]], device='cuda:0')
  File "/home/zl3466/Documents/cmu/10623/proj/train.py", line 123, in <module>
    main()
  File "/home/zl3466/Documents/cmu/10623/proj/train.py", line 118, in main
    trainer.train()
  File "/home/zl3466/Documents/cmu/10623/proj/src/vo_trainer.py", line 270, in train
    trainer.train()
  File "/home/zl3466/anaconda3/envs/voformer/lib/python3.9/site-packages/transformers/trainer.py", line 2325, in train
    return inner_training_loop(
  File "/home/zl3466/anaconda3/envs/voformer/lib/python3.9/site-packages/transformers/trainer.py", line 2674, in _inner_training_loop
    tr_loss_step = self.training_step(model, inputs, num_items_in_batch)
  File "/home/zl3466/anaconda3/envs/voformer/lib/python3.9/site-packages/transformers/trainer.py", line 4020, in training_step
    loss = self.compute_loss(model, inputs, num_items_in_batch=num_items_in_batch)
  File "/home/zl3466/Documents/cmu/10623/proj/src/vo_trainer.py", line 40, in compute_loss
    outputs = model(**inputs)
  File "/home/zl3466/anaconda3/envs/voformer/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/zl3466/anaconda3/envs/voformer/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/zl3466/Documents/cmu/10623/proj/src/vo_model.py", line 63, in forward
    outputs = self.base_model(
  File "/home/zl3466/anaconda3/envs/voformer/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/zl3466/anaconda3/envs/voformer/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/zl3466/anaconda3/envs/voformer/lib/python3.9/site-packages/transformers/models/qwen2_5_vl/modeling_qwen2_5_vl.py", line 1259, in forward
    image_mask, _ = self.get_placeholder_mask(
  File "/home/zl3466/anaconda3/envs/voformer/lib/python3.9/site-packages/transformers/models/qwen2_5_vl/modeling_qwen2_5_vl.py", line 1202, in get_placeholder_mask
    raise ValueError(
ValueError: Image features and image tokens do not match: tokens: 0, features 729
