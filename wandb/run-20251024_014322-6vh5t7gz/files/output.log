  0%|                                                                                                                                                                                  | 0/1860 [00:00<?, ?it/s]Traceback (most recent call last):
Number of input images: 9
Processed pixel_values shape: torch.Size([2916, 1176])
Data collator - pixel_values shape: torch.Size([1, 2916, 1176])
Number of input images: 9
Processed pixel_values shape: torch.Size([2916, 1176])
Data collator - pixel_values shape: torch.Size([1, 2916, 1176])
{'pixel_values': tensor([[[ 0.5581,  0.5727,  0.6165,  ...,  1.0367,  1.0225,  1.1221],
         [ 0.5143,  0.5289,  0.5435,  ..., -0.9256, -0.8688, -0.7550],
         [ 0.6019,  0.6019,  0.6019,  ...,  1.1221,  1.1078,  1.1363],
         ...,
         [-0.3324, -0.3762, -0.3762,  ..., -0.2146, -0.2573, -0.2004],
         [-0.3178, -0.3178, -0.3470,  ..., -0.2004, -0.1435, -0.1293],
         [-0.4054, -0.4200, -0.4346,  ..., -0.2573, -0.2573, -0.2004]]],
       device='cuda:0'), 'input_ids': tensor([[582, 630, 497, 541, 565, 498, 584, 633, 497, 584, 636, 497, 625, 701,
         497, 583, 633, 497, 542, 566, 498, 585, 633, 497]], device='cuda:0'), 'labels': tensor([[542, 568, 498, 585, 636, 497, 584, 635, 497, 541, 568, 498, 583, 636,
         497, 580, 632, 497, 540, 565, 498, 580, 632, 497, 582, 636, 497, 541,
         567, 498, 583, 634, 497, 582, 635, 497, 540, 565, 498, 580, 634, 497,
         581, 634, 497, 540, 568, 498]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]],
       device='cuda:0')}
Model inputs:
  pixel_values shape: torch.Size([1, 2916, 1176])
  input_ids shape: torch.Size([1, 24])
  labels shape: torch.Size([1, 48])
  attention_mask shape: torch.Size([1, 24])
Image grid THW shape: torch.Size([1, 3])
Image grid THW: tensor([[ 1, 54, 54]], device='cuda:0')
Error in base_model forward pass: Image features and image tokens do not match: tokens: 0, features 729
pixel_values shape: torch.Size([1, 2916, 1176])
image_grid_thw shape: torch.Size([1, 3])
input_ids shape: torch.Size([1, 24])
  File "/home/zl3466/Documents/cmu/10623/proj/train.py", line 123, in <module>
    main()
  File "/home/zl3466/Documents/cmu/10623/proj/train.py", line 118, in main
    trainer.train()
  File "/home/zl3466/Documents/cmu/10623/proj/src/vo_trainer.py", line 272, in train
    trainer.train()
  File "/home/zl3466/anaconda3/envs/voformer/lib/python3.9/site-packages/transformers/trainer.py", line 2325, in train
    return inner_training_loop(
  File "/home/zl3466/anaconda3/envs/voformer/lib/python3.9/site-packages/transformers/trainer.py", line 2674, in _inner_training_loop
    tr_loss_step = self.training_step(model, inputs, num_items_in_batch)
  File "/home/zl3466/anaconda3/envs/voformer/lib/python3.9/site-packages/transformers/trainer.py", line 4020, in training_step
    loss = self.compute_loss(model, inputs, num_items_in_batch=num_items_in_batch)
  File "/home/zl3466/Documents/cmu/10623/proj/src/vo_trainer.py", line 40, in compute_loss
    outputs = model(**inputs)
  File "/home/zl3466/anaconda3/envs/voformer/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/zl3466/anaconda3/envs/voformer/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/zl3466/Documents/cmu/10623/proj/src/vo_model.py", line 112, in forward
    raise e
  File "/home/zl3466/Documents/cmu/10623/proj/src/vo_model.py", line 100, in forward
    outputs = self.base_model(
  File "/home/zl3466/anaconda3/envs/voformer/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/zl3466/anaconda3/envs/voformer/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/zl3466/anaconda3/envs/voformer/lib/python3.9/site-packages/transformers/models/qwen2_5_vl/modeling_qwen2_5_vl.py", line 1259, in forward
    image_mask, _ = self.get_placeholder_mask(
  File "/home/zl3466/anaconda3/envs/voformer/lib/python3.9/site-packages/transformers/models/qwen2_5_vl/modeling_qwen2_5_vl.py", line 1202, in get_placeholder_mask
    raise ValueError(
ValueError: Image features and image tokens do not match: tokens: 0, features 729
