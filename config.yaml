# Visual Odometry Training Configuration

# Model configuration
model:
  name: "Qwen/Qwen2.5-0.5B"  # Changed to Qwen 0.5B
  vocab_size: 1000

# Vision configuration for DINOv2
vision:
  dinov2_model: "facebook/dinov2-base"  # DINOv2 model for vision processing
  hidden_size: 768  # DINOv2 hidden size
  num_patches_per_image: 196  # 14x14 patches for 224x224 images

# Data configuration
data:
  dir: "/home/zl3466/Documents/dataset/NuScenes"
  num_input_frames: 8      # 8 input translation deltas (requires 9 images)
  num_input_poses: 8       # 8 input translation deltas (requires 9 poses)
  num_target_poses: 32     # 16 target translation deltas to predict (requires 17 poses)
  train_split: 0.99        # Fraction of data to use for training (0.95 = 95% train, 5% val)

# Training configuration
training:
  output_dir: "./vo_output"
  num_epochs: 2
  batch_size: 2
  learning_rate: 1e-5
  warmup_steps: 100
  logging_steps: 10
  save_steps: 1000
  eval_steps: 1000
  max_grad_norm: 1.0
  weight_decay: 0.01
  use_wandb: true
  wandb_project: "visual-odometry"
  wandb_run_name: "qwen25-0.5B-translation-deltas"

# Pose configuration
pose:
  num_joints: 1  # Only 1 translation vector (x, y, z)
  pose_dim: 3    # 3D translation (x, y, z)
  num_tokens_pose: 3  # 3 tokens for (x, y, z)
  quantization_range: 2.0  # Range for translation deltas
  pose_representation: "translation"

# Image processing configuration
image:
  input_size: 256  # Changed to 256 to get 16x16 = 256 tokens (perfect square)
  num_tokens: 64
  # patch_size: 16  # Patch size for Qwen2.5-VL vision encoder
