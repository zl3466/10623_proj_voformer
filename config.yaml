# Visual Odometry Training Configuration

# Model configuration
model:
  name: "Qwen/Qwen2.5-VL-3B-Instruct"
  vocab_size: 1000

# Data configuration
data:
  dir: "/home/zl3466/Documents/dataset/NuScenes"
  num_input_frames: 8      # 8 input translation deltas (requires 9 images)
  num_input_poses: 8       # 8 input translation deltas (requires 9 poses)
  num_target_poses: 16     # 16 target translation deltas to predict (requires 17 poses)

# Training configuration
training:
  output_dir: "./vo_output"
  num_epochs: 10
  batch_size: 1
  learning_rate: 1e-5
  warmup_steps: 100
  logging_steps: 10
  save_steps: 500
  eval_steps: 500
  max_grad_norm: 1.0
  weight_decay: 0.01
  use_wandb: false
  wandb_project: "visual-odometry"
  wandb_run_name: "qwen25-vl-3b-translation-deltas"

# Pose configuration
pose:
  num_joints: 1  # Only 1 translation vector (x, y, z)
  pose_dim: 3    # 3D translation (x, y, z)
  num_tokens_pose: 3  # 3 tokens for (x, y, z)
  quantization_range: 2.0  # Range for translation deltas
  pose_representation: "translation"

# Image processing configuration
image:
  input_size: 256  # Changed to 256 to get 16x16 = 256 tokens (perfect square)
  num_tokens: 144
  # patch_size: 16  # Patch size for Qwen2.5-VL vision encoder
